{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd          \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2`' \n",
    "\n",
    "learning_rate = 0.01\n",
    "num_steps = 10\n",
    "mini_batch_size = 10\n",
    "display_step = 1\n",
    "\n",
    "n_hidden_1  = 8\n",
    "n_hidden_2  = 8\n",
    "n_hidden_3 = 8\n",
    "num_input  = 13\n",
    "num_classes = 2\n",
    "\n",
    "output_file = open(\"C:/Users/Ishita Jain/Desktop/output.txt\",\"w+\")\n",
    "\n",
    "# Might consider making changes to evaluation or minimization\n",
    "# Standard Deviation of 0.02 works 13 13 8 or 13 13 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    preds_correct_boolean = np.equal(predictions,labels)\n",
    "    print(preds_correct_boolean)\n",
    "    correct_predictions = np.sum(preds_correct_boolean)\n",
    "    print(correct_predictions)\n",
    "    acc = 100.0 * (correct_predictions / preds_correct_boolean.shape[0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Logistic(X,T):\n",
    "    logistic = LogisticRegression()\n",
    "    a = logistic.fit(X,T)\n",
    "    y_pred = logistic.predict(X)\n",
    "    ac = accuracy_score(y_pred,T)\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn(X,T2,X_test,Y_test):\n",
    "    \n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, num_input])\n",
    "    y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1],dtype=np.float32,stddev = 0.002)),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=np.float32,stddev = 0.002)),\t\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes], dtype=np.float32,stddev = 0.002))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1], dtype=np.float32,stddev = 0.002)),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2], dtype=np.float32,stddev = 0.002)),\n",
    "        'out': tf.Variable(tf.random_normal([num_classes], dtype=np.float32,stddev = 0.002))\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    y_layer_1 = tf.nn.relu(layer_1) #tf.nn.sigmoid\n",
    "    layer_2 = tf.add(tf.matmul(y_layer_1, weights['h2']), biases['b2'])\n",
    "    y_layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(y_layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    # Construct model\n",
    "    logits = out_layer\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "    #loss_op = tf.reduce_mean(-tf.reduce_sum(y * tf.log(prediction), reduction_indices=[1]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    #grads_and_vars = optimizer.compute_gradients(loss_op,tf.trainable_variables())\n",
    "    #train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model\n",
    "    qq = tf.argmax(prediction, 1)\n",
    "    correct_pred = tf.equal(qq, tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    t_acc = 0\n",
    "\n",
    "    # Start training\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "        fin_acc = 0\n",
    "        for step in range(1, num_steps+1):\n",
    "            offset = (step * mini_batch_size) % (X.shape[0] - mini_batch_size)\n",
    "            batch_x = X[offset:(offset + mini_batch_size), :]\n",
    "            batch_y = T2[offset:(offset + mini_batch_size)]\n",
    "\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "            #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={x:batch_x, y : batch_y})\n",
    "            #print(gr_print)\n",
    "\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={x: batch_x,y: batch_y})\n",
    "\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.9f}\".format(loss) + \", Training Accuracy= \" + \"{:.9f}\".format(acc))\n",
    "\n",
    "\n",
    "        fin_acc = sess.run(accuracy, feed_dict={x: X,y: T2})\n",
    "        writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "\n",
    "        print(\"Training Accuracy is\")\n",
    "        print(fin_acc)\n",
    "        print(\"Testing Accuracy is\")\n",
    "        t_acc = sess.run(accuracy,feed_dict = {x:X_test,y:Y_test})\n",
    "        print(t_acc)\n",
    "\n",
    "    return t_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression is :\n",
      "0.7889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishita jain\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.263055891, Training Accuracy= 1.000000000\n",
      "Step 2, Minibatch Loss= 0.501988769, Training Accuracy= 0.800000012\n",
      "Step 3, Minibatch Loss= 0.927805901, Training Accuracy= 0.699999988\n",
      "Step 4, Minibatch Loss= 0.523442447, Training Accuracy= 0.899999976\n",
      "Step 5, Minibatch Loss= 0.673627675, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.673686028, Training Accuracy= 0.800000012\n",
      "Step 7, Minibatch Loss= 0.687204957, Training Accuracy= 0.600000024\n",
      "Step 8, Minibatch Loss= 0.678873003, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.667965889, Training Accuracy= 0.800000012\n",
      "Step 10, Minibatch Loss= 0.653864563, Training Accuracy= 0.899999976\n",
      "Training Accuracy is\n",
      "0.797375\n",
      "Testing Accuracy is\n",
      "0.792\n",
      "Step 1, Minibatch Loss= 0.536873579, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.803692162, Training Accuracy= 0.300000012\n",
      "Step 3, Minibatch Loss= 0.627812505, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.681042075, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.681611657, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.655423284, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.657730877, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.673217595, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.645453990, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.667300344, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.799125\n",
      "Testing Accuracy is\n",
      "0.785\n",
      "Step 1, Minibatch Loss= 0.492496789, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.735412061, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.552481234, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.689508975, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.681777596, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.664614081, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.666363597, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.677994251, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.657042384, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.673243463, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.79575\n",
      "Testing Accuracy is\n",
      "0.7985\n",
      "Step 1, Minibatch Loss= 0.332760453, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.504599333, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.640097320, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.688580155, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.680764616, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.654210329, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.655829608, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.672191322, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.643181622, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.666384578, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.795125\n",
      "Testing Accuracy is\n",
      "0.801\n",
      "Step 1, Minibatch Loss= 0.375959784, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.631913543, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.519194007, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.688032866, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.686257839, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.672006190, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.672201633, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.680782378, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.662921071, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.676040769, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.794125\n",
      "Testing Accuracy is\n",
      "0.805\n",
      "Step 1, Minibatch Loss= 0.772087216, Training Accuracy= 0.699999988\n",
      "Step 2, Minibatch Loss= 0.520017982, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.116742536, Training Accuracy= 1.000000000\n",
      "Step 4, Minibatch Loss= 0.713178933, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.549960375, Training Accuracy= 0.899999976\n",
      "Step 6, Minibatch Loss= 0.558437169, Training Accuracy= 0.500000000\n",
      "Step 7, Minibatch Loss= 0.469470024, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.678384483, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.668101490, Training Accuracy= 0.699999988\n",
      "Step 10, Minibatch Loss= 0.653038323, Training Accuracy= 0.899999976\n",
      "Training Accuracy is\n",
      "0.7963333\n",
      "Testing Accuracy is\n",
      "0.796\n",
      "Step 1, Minibatch Loss= 0.464766920, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.557412148, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.563947856, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.692680955, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.680372715, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.670688689, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.671070039, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.680150628, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.661314368, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.675089777, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.7972222\n",
      "Testing Accuracy is\n",
      "0.788\n",
      "Step 1, Minibatch Loss= 0.455791712, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.477470100, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.534283042, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.687943339, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.681044698, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.657217205, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.658272088, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.673660398, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.646938145, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.668308437, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.79766667\n",
      "Testing Accuracy is\n",
      "0.784\n",
      "Step 1, Minibatch Loss= 0.361703455, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.465630352, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.533387661, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.695707321, Training Accuracy= 0.500000000\n",
      "Step 5, Minibatch Loss= 0.684622765, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.666712284, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.667366326, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.678212702, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.657076955, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.673139393, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.79744446\n",
      "Testing Accuracy is\n",
      "0.786\n",
      "Step 1, Minibatch Loss= 0.773396850, Training Accuracy= 0.100000001\n",
      "Step 2, Minibatch Loss= 0.484201044, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.776724696, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.686937511, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.677861989, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.646543801, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.650039494, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.669670880, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.638036907, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.664248705, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.7958889\n",
      "Testing Accuracy is\n",
      "0.8\n",
      "Step 1, Minibatch Loss= 0.333832920, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.406475961, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.591984153, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 1.654949427, Training Accuracy= 0.400000006\n",
      "Step 5, Minibatch Loss= 0.772572875, Training Accuracy= 0.200000003\n",
      "Step 6, Minibatch Loss= 0.651939452, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.654279709, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.671618104, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.643016517, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.666306973, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.7962222\n",
      "Testing Accuracy is\n",
      "0.797\n",
      "Step 1, Minibatch Loss= 0.578508258, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.352687776, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.551190853, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.714093089, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.668742299, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.661770940, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.664281666, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.676943421, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.654780686, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.672192216, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.7952222\n",
      "Testing Accuracy is\n",
      "0.806\n",
      "Step 1, Minibatch Loss= 0.362667739, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.481930166, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.662087262, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.698525667, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.676589310, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.664470553, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.665886045, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.677564025, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.655868351, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.672651052, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.7963333\n",
      "Testing Accuracy is\n",
      "0.796\n",
      "Step 1, Minibatch Loss= 0.427580118, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.650033355, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.593377709, Training Accuracy= 0.800000012\n",
      "Step 4, Minibatch Loss= 0.689001203, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.680281222, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.653527975, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.655785680, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.672369242, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.643958628, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.666894495, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.7965556\n",
      "Testing Accuracy is\n",
      "0.789\n",
      "Step 1, Minibatch Loss= 0.346598893, Training Accuracy= 0.899999976\n",
      "Step 2, Minibatch Loss= 0.414396763, Training Accuracy= 0.899999976\n",
      "Step 3, Minibatch Loss= 0.933673561, Training Accuracy= 0.600000024\n",
      "Step 4, Minibatch Loss= 3.809524059, Training Accuracy= 0.600000024\n",
      "Step 5, Minibatch Loss= 0.748283744, Training Accuracy= 0.699999988\n",
      "Step 6, Minibatch Loss= 0.050313167, Training Accuracy= 1.000000000\n",
      "Step 7, Minibatch Loss= 0.308976948, Training Accuracy= 0.899999976\n",
      "Step 8, Minibatch Loss= 0.688021243, Training Accuracy= 0.699999988\n",
      "Step 9, Minibatch Loss= 0.479696363, Training Accuracy= 0.899999976\n",
      "Step 10, Minibatch Loss= 0.651992083, Training Accuracy= 0.699999988\n",
      "Training Accuracy is\n",
      "0.79355556\n",
      "Testing Accuracy is\n",
      "0.821\n",
      "[0.7963, 0.7963]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/Ishita Jain/Desktop/Bank_EXIT_Survey.csv\")\n",
    "feature_names = list(data)\n",
    "\n",
    "data = data[feature_names[3:]]\n",
    "target = data[\"Status\"]\n",
    "data.drop(\"Status\",axis=1,inplace=True)\n",
    "\n",
    "data2 = pd.get_dummies(data[\"City\"])\n",
    "result = pd.concat([data, data2], axis=1)\n",
    "result.drop(\"City\",axis=1,inplace=True)\n",
    "\n",
    "data2 = pd.get_dummies(result[\"Gender\"])\n",
    "result = pd.concat([result,data2],axis=1)\n",
    "result.drop(\"Gender\",axis=1,inplace=True)\n",
    "\n",
    "X = result.values\n",
    "T = target.values\n",
    "\n",
    "res = tf.one_hot(indices = T,depth = 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    T2 = res.eval()\n",
    "\n",
    "print(\"Accuracy for Logistic Regression is :\")\n",
    "print(Logistic(X,T))\n",
    "\n",
    "k_fold_list = [5,10]\n",
    "mean_accuracies = []\n",
    "\n",
    "for k_fold in k_fold_list:\n",
    "\n",
    "    output_file.write(\"For \" + str(k_fold) + \" Validation:\\n\")\n",
    "\n",
    "    cv = KFold(n_splits=k_fold)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X):\n",
    "            training_X = X[train_index]\n",
    "            training_y = T2[train_index]\n",
    "            testing_X = X[test_index]\n",
    "            testing_y = T2[test_index]\n",
    "            accuracies.append(nn(training_X,training_y,testing_X,testing_y))\n",
    "\n",
    "    output_file.write(\"The accuracies are:\\n\")\n",
    "    output_file.write(str(accuracies) + \"\\n\")\n",
    "\n",
    "    output_file.write(\"The mean accuracy is :\")\n",
    "    mean_accuracies.append(np.mean(accuracies))\n",
    "    output_file.write(str(np.mean(accuracies)))\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "print(mean_accuracies)\n",
    "output_file.write(\"Mean accuracies are \")\n",
    "output_file.write(str(mean_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditBonus</th>\n",
       "      <th>City</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>AvailedProducts</th>\n",
       "      <th>CreditCardPresent</th>\n",
       "      <th>ActiveOnline</th>\n",
       "      <th>Current Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>645</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>822</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>376</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>501</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>684</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>528</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>497</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>476</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>549</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>635</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>616</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>653</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>549</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>587</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>726</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>732</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170886.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>636</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138555.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>510</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118913.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>669</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8487.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>846</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>187616.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>577</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>124508.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>756</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>136815.64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170041.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>571</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38433.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>574</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>141349.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100187.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>411</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>59697.17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53483.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>518</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>151027.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119377.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>833</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>144751.81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166472.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>758</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>155739.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171552.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>611</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157474.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>583</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>122531.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13549.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>610</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>113957.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>196526.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>637</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>103377.81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84419.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>683</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24991.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>774</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>93017.47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>191608.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>677</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>90022.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2988.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>741</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>74371.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99595.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>498</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>152039.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53445.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>655</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>137145.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115146.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>613</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151325.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>602</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>90602.42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51695.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>659</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>673</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>183579.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34047.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>606</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>180307.73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1914.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>775</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49337.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>841</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>179436.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>714</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>35016.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53667.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>597</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>88381.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69384.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>726</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195192.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>644</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>155060.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29179.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>800</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>Goa</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>Pilani</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditBonus       City  Gender  Age  Tenure    Balance  AvailedProducts  \\\n",
       "0             619     Pilani  Female   42       2       0.00                1   \n",
       "1             608  Hyderabad  Female   41       1   83807.86                1   \n",
       "2             502     Pilani  Female   42       8  159660.80                3   \n",
       "3             699     Pilani  Female   39       1       0.00                2   \n",
       "4             850  Hyderabad  Female   43       2  125510.82                1   \n",
       "5             645  Hyderabad    Male   44       8  113755.78                2   \n",
       "6             822     Pilani    Male   50       7       0.00                2   \n",
       "7             376        Goa  Female   29       4  115046.74                4   \n",
       "8             501     Pilani    Male   44       4  142051.07                2   \n",
       "9             684     Pilani    Male   27       2  134603.88                1   \n",
       "10            528     Pilani    Male   31       6  102016.72                2   \n",
       "11            497  Hyderabad    Male   24       3       0.00                2   \n",
       "12            476     Pilani  Female   34      10       0.00                2   \n",
       "13            549     Pilani  Female   25       5       0.00                2   \n",
       "14            635  Hyderabad  Female   35       7       0.00                2   \n",
       "15            616        Goa    Male   45       3  143129.41                2   \n",
       "16            653        Goa    Male   58       1  132602.88                1   \n",
       "17            549  Hyderabad  Female   24       9       0.00                2   \n",
       "18            587  Hyderabad    Male   45       6       0.00                1   \n",
       "19            726     Pilani  Female   24       6       0.00                2   \n",
       "20            732     Pilani    Male   41       8       0.00                2   \n",
       "21            636  Hyderabad  Female   32       8       0.00                2   \n",
       "22            510  Hyderabad  Female   38       4       0.00                1   \n",
       "23            669     Pilani    Male   46       3       0.00                2   \n",
       "24            846     Pilani  Female   38       5       0.00                1   \n",
       "25            577     Pilani    Male   25       3       0.00                2   \n",
       "26            756        Goa    Male   36       2  136815.64                1   \n",
       "27            571     Pilani    Male   44       9       0.00                2   \n",
       "28            574        Goa  Female   43       3  141349.43                1   \n",
       "29            411     Pilani    Male   29       0   59697.17                2   \n",
       "...           ...        ...     ...  ...     ...        ...              ...   \n",
       "9970          518     Pilani    Male   42       7  151027.05                2   \n",
       "9971          833     Pilani  Female   34       3  144751.81                1   \n",
       "9972          758     Pilani    Male   26       4  155739.76                1   \n",
       "9973          611     Pilani    Male   27       7       0.00                2   \n",
       "9974          583     Pilani    Male   33       7  122531.86                1   \n",
       "9975          610        Goa    Male   50       1  113957.01                2   \n",
       "9976          637     Pilani  Female   33       7  103377.81                1   \n",
       "9977          683     Pilani  Female   32       9       0.00                2   \n",
       "9978          774     Pilani    Male   40       9   93017.47                2   \n",
       "9979          677     Pilani  Female   58       1   90022.85                1   \n",
       "9980          741  Hyderabad    Male   35       6   74371.49                1   \n",
       "9981          498        Goa    Male   42       3  152039.70                1   \n",
       "9982          655        Goa  Female   46       7  137145.12                1   \n",
       "9983          613     Pilani    Male   40       4       0.00                1   \n",
       "9984          602        Goa    Male   35       7   90602.42                2   \n",
       "9985          659     Pilani    Male   36       6  123841.49                2   \n",
       "9986          673        Goa    Male   47       1  183579.54                2   \n",
       "9987          606  Hyderabad    Male   30       8  180307.73                2   \n",
       "9988          775     Pilani    Male   30       4       0.00                2   \n",
       "9989          841  Hyderabad    Male   28       4       0.00                2   \n",
       "9990          714        Goa    Male   33       3   35016.60                1   \n",
       "9991          597     Pilani  Female   53       4   88381.21                1   \n",
       "9992          726  Hyderabad    Male   36       2       0.00                1   \n",
       "9993          644     Pilani    Male   28       7  155060.41                1   \n",
       "9994          800     Pilani  Female   29       2       0.00                2   \n",
       "9995          771     Pilani    Male   39       5       0.00                2   \n",
       "9996          516     Pilani    Male   35      10   57369.61                1   \n",
       "9997          709     Pilani  Female   36       7       0.00                1   \n",
       "9998          772        Goa    Male   42       3   75075.31                2   \n",
       "9999          792     Pilani  Female   28       4  130142.79                1   \n",
       "\n",
       "      CreditCardPresent  ActiveOnline  Current Salary  \n",
       "0                     1             1       101348.88  \n",
       "1                     0             1       112542.58  \n",
       "2                     1             0       113931.57  \n",
       "3                     0             0        93826.63  \n",
       "4                     1             1        79084.10  \n",
       "5                     1             0       149756.71  \n",
       "6                     1             1        10062.80  \n",
       "7                     1             0       119346.88  \n",
       "8                     0             1        74940.50  \n",
       "9                     1             1        71725.73  \n",
       "10                    0             0        80181.12  \n",
       "11                    1             0        76390.01  \n",
       "12                    1             0        26260.98  \n",
       "13                    0             0       190857.79  \n",
       "14                    1             1        65951.65  \n",
       "15                    0             1        64327.26  \n",
       "16                    1             0         5097.67  \n",
       "17                    1             1        14406.41  \n",
       "18                    0             0       158684.81  \n",
       "19                    1             1        54724.03  \n",
       "20                    1             1       170886.17  \n",
       "21                    1             0       138555.46  \n",
       "22                    1             0       118913.53  \n",
       "23                    0             1         8487.75  \n",
       "24                    1             1       187616.16  \n",
       "25                    0             1       124508.29  \n",
       "26                    1             1       170041.95  \n",
       "27                    0             0        38433.35  \n",
       "28                    1             1       100187.43  \n",
       "29                    1             1        53483.21  \n",
       "...                 ...           ...             ...  \n",
       "9970                  1             0       119377.36  \n",
       "9971                  0             0       166472.81  \n",
       "9972                  1             0       171552.02  \n",
       "9973                  1             1       157474.10  \n",
       "9974                  1             0        13549.24  \n",
       "9975                  1             0       196526.55  \n",
       "9976                  1             0        84419.78  \n",
       "9977                  1             1        24991.92  \n",
       "9978                  1             0       191608.97  \n",
       "9979                  0             1         2988.28  \n",
       "9980                  0             0        99595.67  \n",
       "9981                  1             1        53445.17  \n",
       "9982                  1             0       115146.40  \n",
       "9983                  0             0       151325.24  \n",
       "9984                  1             1        51695.41  \n",
       "9985                  1             0        96833.00  \n",
       "9986                  0             1        34047.54  \n",
       "9987                  1             1         1914.41  \n",
       "9988                  1             0        49337.84  \n",
       "9989                  1             1       179436.60  \n",
       "9990                  1             0        53667.08  \n",
       "9991                  1             0        69384.71  \n",
       "9992                  1             0       195192.40  \n",
       "9993                  1             0        29179.52  \n",
       "9994                  0             0       167773.55  \n",
       "9995                  1             0        96270.64  \n",
       "9996                  1             1       101699.77  \n",
       "9997                  0             1        42085.58  \n",
       "9998                  1             0        92888.52  \n",
       "9999                  1             0        38190.78  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=[606,\"Pilani\",\"Male\",23,5,75000,2,1,1,25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dc0b92010060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logistic' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = logistic.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Goa  Hyderabad  Pilani\n",
      "0       0          0       1\n",
      "1       0          1       0\n",
      "2       0          0       1\n",
      "3       0          0       1\n",
      "4       0          1       0\n",
      "5       0          1       0\n",
      "6       0          0       1\n",
      "7       1          0       0\n",
      "8       0          0       1\n",
      "9       0          0       1\n",
      "10      0          0       1\n",
      "11      0          1       0\n",
      "12      0          0       1\n",
      "13      0          0       1\n",
      "14      0          1       0\n",
      "15      1          0       0\n",
      "16      1          0       0\n",
      "17      0          1       0\n",
      "18      0          1       0\n",
      "19      0          0       1\n",
      "20      0          0       1\n",
      "21      0          1       0\n",
      "22      0          1       0\n",
      "23      0          0       1\n",
      "24      0          0       1\n",
      "25      0          0       1\n",
      "26      1          0       0\n",
      "27      0          0       1\n",
      "28      1          0       0\n",
      "29      0          0       1\n",
      "...   ...        ...     ...\n",
      "9970    0          0       1\n",
      "9971    0          0       1\n",
      "9972    0          0       1\n",
      "9973    0          0       1\n",
      "9974    0          0       1\n",
      "9975    1          0       0\n",
      "9976    0          0       1\n",
      "9977    0          0       1\n",
      "9978    0          0       1\n",
      "9979    0          0       1\n",
      "9980    0          1       0\n",
      "9981    1          0       0\n",
      "9982    1          0       0\n",
      "9983    0          0       1\n",
      "9984    1          0       0\n",
      "9985    0          0       1\n",
      "9986    1          0       0\n",
      "9987    0          1       0\n",
      "9988    0          0       1\n",
      "9989    0          1       0\n",
      "9990    1          0       0\n",
      "9991    0          0       1\n",
      "9992    0          1       0\n",
      "9993    0          0       1\n",
      "9994    0          0       1\n",
      "9995    0          0       1\n",
      "9996    0          0       1\n",
      "9997    0          0       1\n",
      "9998    1          0       0\n",
      "9999    0          0       1\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "      Female  Male\n",
      "0          1     0\n",
      "1          1     0\n",
      "2          1     0\n",
      "3          1     0\n",
      "4          1     0\n",
      "5          0     1\n",
      "6          0     1\n",
      "7          1     0\n",
      "8          0     1\n",
      "9          0     1\n",
      "10         0     1\n",
      "11         0     1\n",
      "12         1     0\n",
      "13         1     0\n",
      "14         1     0\n",
      "15         0     1\n",
      "16         0     1\n",
      "17         1     0\n",
      "18         0     1\n",
      "19         1     0\n",
      "20         0     1\n",
      "21         1     0\n",
      "22         1     0\n",
      "23         0     1\n",
      "24         1     0\n",
      "25         0     1\n",
      "26         0     1\n",
      "27         0     1\n",
      "28         1     0\n",
      "29         0     1\n",
      "...      ...   ...\n",
      "9970       0     1\n",
      "9971       1     0\n",
      "9972       0     1\n",
      "9973       0     1\n",
      "9974       0     1\n",
      "9975       0     1\n",
      "9976       1     0\n",
      "9977       1     0\n",
      "9978       0     1\n",
      "9979       1     0\n",
      "9980       0     1\n",
      "9981       0     1\n",
      "9982       1     0\n",
      "9983       0     1\n",
      "9984       0     1\n",
      "9985       0     1\n",
      "9986       0     1\n",
      "9987       0     1\n",
      "9988       0     1\n",
      "9989       0     1\n",
      "9990       0     1\n",
      "9991       1     0\n",
      "9992       0     1\n",
      "9993       0     1\n",
      "9994       1     0\n",
      "9995       0     1\n",
      "9996       0     1\n",
      "9997       1     0\n",
      "9998       0     1\n",
      "9999       1     0\n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "[6.0800000e+02 4.1000000e+01 1.0000000e+00 8.3807860e+04 1.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 1.1254258e+05 0.0000000e+00 1.0000000e+00\n",
      " 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/Ishita Jain/Desktop/Bank_EXIT_Survey.csv\")\n",
    "feature_names = list(data)\n",
    "\n",
    "data = data[feature_names[3:]]\n",
    "target = data[\"Status\"]\n",
    "data.drop(\"Status\",axis=1,inplace=True)\n",
    "\n",
    "data2 = pd.get_dummies(data[\"City\"])\n",
    "print(data2)\n",
    "result = pd.concat([data, data2], axis=1)\n",
    "result.drop(\"City\",axis=1,inplace=True)\n",
    "\n",
    "data2 = pd.get_dummies(result[\"Gender\"])\n",
    "print(data2)\n",
    "result = pd.concat([result,data2],axis=1)\n",
    "result.drop(\"Gender\",axis=1,inplace=True)\n",
    "\n",
    "X = result.values\n",
    "print(X[1])\n",
    "T = target.values\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[606, 0, 0, 1, 0, 1, 23, 5, 75000, 2, 1, 1, 25000]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-256b0c6e28f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;31m#x=[606 0 0 1 0 1 23 5 75000 2 1 1 25000]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ishita jain\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ishita jain\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coef_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             raise NotFittedError(\"This %(name)s instance is not fitted \"\n\u001b[0;32m--> 255\u001b[0;31m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "x.append(606)\n",
    "x.append(0)\n",
    "x.append(0)\n",
    "x.append(1)\n",
    "x.append(0)\n",
    "x.append(1)\n",
    "x.append(23)\n",
    "x.append(5)\n",
    "x.append(75000)\n",
    "x.append(2)\n",
    "x.append(1)\n",
    "x.append(1)\n",
    "x.append(25000)\n",
    "print(x)\n",
    "#x=[606 0 0 1 0 1 23 5 75000 2 1 1 25000]\n",
    "\n",
    "y_pred = logistic.predict(X)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
